#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ¥å‘Šä¿¡æ¯æ”¶é›†å™¨ - ä»CSVæ–‡ä»¶è¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®èµ„æºä¿¡æ¯

åŠŸèƒ½ï¼š
1. ä»CSVæ–‡ä»¶ä¸­è¯»å–PDFé“¾æ¥
2. é€ä¸ªè§£æPDFï¼Œæå–"å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"çš„æ•°æ®
3. ç”Ÿæˆé•¿æ ¼å¼å’Œå®½æ ¼å¼çš„ExcelæŠ¥å‘Š
4. æ·»åŠ "æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"æ ‡è®°åˆ—
"""

import os
import re
import requests
import pandas as pd
import pdfplumber
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from datetime import datetime
import warnings
import logging
from pathlib import Path
import glob
import argparse
from decimal import Decimal, InvalidOperation

# æŠ‘åˆ¶pdfplumberçš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")
logging.getLogger("pdfplumber").setLevel(logging.ERROR)


TARGET_KEYWORD = "å…¶ä¸­ï¼šæ•°æ®èµ„æº"
PARENT_CATEGORIES = ["å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"]
SPECIAL_UNIT_MULTIPLIERS = {
    "600941.SH": (Decimal("1000000"), "ç™¾ä¸‡"),
    "601727.SH": (Decimal("1000"), "åƒ"),
}


def _normalize_text(text: str) -> str:
    """ç»Ÿä¸€æ–‡æœ¬ï¼šå»æ‰æ¢è¡Œã€ç©ºæ ¼ï¼Œå¹¶å°†è‹±æ–‡å†’å·æ›¿æ¢ä¸ºä¸­æ–‡å†’å·ã€‚"""
    if text is None:
        return ""
    cleaned = str(text).replace('\n', '')
    cleaned = cleaned.replace(':', 'ï¼š')
    cleaned = re.sub(r'\s+', '', cleaned)
    return cleaned


def adjust_amount_for_special_unit(sec_code: str, amount_str: str) -> str:
    """é’ˆå¯¹ç‰¹æ®Šè¯åˆ¸ä»£ç æŒ‰å•ä½æ¢ç®—é‡‘é¢"""
    if not amount_str:
        return amount_str

    normalized_code = (sec_code or "").upper()
    config = SPECIAL_UNIT_MULTIPLIERS.get(normalized_code)
    if not config:
        return amount_str

    multiplier, unit_label = config
    cleaned_amount = (
        str(amount_str).replace(",", "").replace(" ", "").strip()
    )
    if cleaned_amount in {"", "N/A", "ç©ºå€¼", "-", "nan", "None"}:
        return amount_str

    try:
        numeric_value = Decimal(cleaned_amount)
    except (InvalidOperation, ValueError):
        return amount_str

    adjusted_value = numeric_value * multiplier
    formatted = f"{adjusted_value:,.2f}".rstrip("0").rstrip(".")
    print(
        f"  ğŸ”„ {normalized_code} å•ä½ä¸º{unit_label}ï¼Œå·²æ¢ç®—é‡‘é¢: {amount_str} -> {formatted}"
    )
    return formatted if formatted else "0"


def extract_data_by_table(pdf_content, pdf_url):
    """
    ä»…é€šè¿‡è¡¨æ ¼æ–¹å¼æŸ¥æ‰¾"å…¶ä¸­ï¼šæ•°æ®èµ„æº"ã€‚
    è§„åˆ™ï¼š
        1. å¿…é¡»å‡ºç°åœ¨åŒä¸€ä¸ªè¡¨æ ¼çš„è¡Œå†…ï¼›
        2. å…è®¸ç›®æ ‡æ–‡å­—åŠçˆ¶ç±»åˆ«æ–‡å­—ä¸­åŒ…å«ç©ºæ ¼ã€å…¨è§’ç©ºæ ¼ã€ä¸åŒå†’å·ï¼›
        3. å–è¯¥è¡Œä¸­â€œå…¶ä¸­ï¼šæ•°æ®èµ„æºâ€ä¹‹åçš„ç¬¬ä¸€ä¸ª >0 çš„æ•°å­—ï¼›
        4. çˆ¶ç±»åˆ«åªå¯èƒ½æ˜¯["å­˜è´§","æ— å½¢èµ„äº§","å¼€å‘æ”¯å‡º"]ï¼Œå–ä¸Šä¸€è¡Œæœ€è¿‘çš„éç©ºå€¼ã€‚
    
    Returns:
        tuple: (æå–ç»“æœåˆ—è¡¨, æ˜¯å¦åœ¨è¡¨æ ¼ä¸­æ‰¾åˆ°"å…¶ä¸­ï¼šæ•°æ®èµ„æº")
    """
    found_items = []
    has_data_resource_keyword = False

    def extract_number_from_text(text):
        """ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°å­—"""
        if not text:
            return None, False, False
        cleaned_text = str(text).strip()
        number_patterns = [
            r'((?:\d{1,3},)*\d{1,3}\.\d{2})',
            r'((?:\d{1,3},)*\d{1,3}\.\d+)',
            r'((?:\d{1,3},)+\d+)',
            r'((?:\d{1,3},)*\d+)',
            r'(\d+\.\d{2})',
            r'(\d+\.\d+)',
            r'(\d+)',
        ]
        for pattern in number_patterns:
            match = re.search(pattern, cleaned_text)
            if match:
                value_str = match.group(1)
                try:
                    numeric_value = float(value_str.replace(',', ''))
                    return value_str, True, numeric_value > 0
                except Exception:
                    return value_str, True, True
        return None, False, False

    def find_parent_category(table, current_index):
        """å‘ä¸ŠæŸ¥æ‰¾çˆ¶ç±»åˆ«ï¼ˆä¸Šä¸€è¡Œï¼Œå…è®¸è·³è¿‡ç©ºè¡Œï¼‰"""
        parent_row_idx = current_index - 1
        while parent_row_idx >= 0:
            parent_row = table[parent_row_idx]
            if not parent_row:
                parent_row_idx -= 1
                continue
            normalized_cells = ''.join(_normalize_text(cell) for cell in parent_row if cell)
            if not normalized_cells:
                parent_row_idx -= 1
                continue
            for cat in PARENT_CATEGORIES:
                if cat in normalized_cells:
                    return cat
            # å¦‚æœä¸Šä¸€è¡Œæœ‰æ–‡æœ¬ä½†ä¸æ˜¯ç›®æ ‡çˆ¶ç±»ï¼Œåˆ™åœæ­¢æŸ¥æ‰¾ï¼Œé¿å…è·¨è¶Šå…¶ä»–æ®µè½
            break
        return None

    try:
        import sys
        from io import StringIO

        old_stderr = sys.stderr
        sys.stderr = StringIO()

        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with pdfplumber.open(BytesIO(pdf_content)) as pdf:
                    for page_num, page in enumerate(pdf.pages, 1):
                        # åªè¦é¡µé¢æ–‡æœ¬ä¸­åŒ…å«"æ•°æ®èµ„æº"ï¼Œå°±æ ‡è®°ä¸ºTrueï¼Œç”¨äº"æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"
                        page_text = page.extract_text() or ""
                        if "æ•°æ®èµ„æº" in page_text:
                            has_data_resource_keyword = True

                        tables = page.extract_tables()
                        if not tables:
                            continue
                        for table in tables:
                            if not table:
                                continue
                            for row_idx, row in enumerate(table):
                                if not row:
                                    continue
                                normalized_cells = [_normalize_text(cell) for cell in row]
                                target_col_idx = None
                                for col_idx, cell_text in enumerate(normalized_cells):
                                    if cell_text and TARGET_KEYWORD in cell_text:
                                        target_col_idx = col_idx
                                        has_data_resource_keyword = True
                                        break
                                if target_col_idx is None:
                                    continue

                                # åœ¨åŒä¸€è¡Œä¸­ï¼Œå¯»æ‰¾ç›®æ ‡æ–‡å­—åçš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°å­—
                                found_value = None
                                for col_idx in range(target_col_idx, len(row)):
                                    cell_value = row[col_idx]
                                    value, has_num, is_positive = extract_number_from_text(cell_value)
                                    if has_num and is_positive:
                                        found_value = value
                                        break
                                if not found_value:
                                    continue

                                parent_category = find_parent_category(table, row_idx)
                                if not parent_category:
                                    continue

                                found_items.append({
                                    "category": parent_category,
                                    "value": found_value,
                                    "method": "table",
                                    "page": page_num
                                })
                                print(f"    âœ… [è¡¨æ ¼] ç¬¬{page_num}é¡µ {parent_category}å…¶ä¸­ï¼šæ•°æ®èµ„æº: {found_value}")
        finally:
            sys.stderr = old_stderr

    except Exception as e:
        print(f"    âš ï¸ è¡¨æ ¼æå–æ–¹æ³•å‡ºé”™: {e}")
        return [], has_data_resource_keyword

    if not found_items:
        print("    âš ï¸ è¡¨æ ¼ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„'å…¶ä¸­ï¼šæ•°æ®èµ„æº'ã€‚")

    return found_items, has_data_resource_keyword


def process_pdf_link(row_data, session, headers, folder_path, download_pdf=True):
    """
    å¤„ç†å•ä¸ªPDFé“¾æ¥ï¼Œä¸‹è½½å¹¶è§£ææ•°æ®
    
    Args:
        row_data (dict): CSVè¡Œæ•°æ®ï¼ŒåŒ…å«PDFé“¾æ¥ç­‰ä¿¡æ¯
        session (requests.Session): è¯·æ±‚ä¼šè¯
        headers (dict): è¯·æ±‚å¤´
        folder_path (str): ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„
        download_pdf (bool): æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°
    
    Returns:
        list: è§£æç»“æœåˆ—è¡¨
    """
    pdf_url = row_data.get('PDFé“¾æ¥', '')
    if not pdf_url:
        print(f"  âŒ è·³è¿‡ï¼šæ— PDFé“¾æ¥")
        return []
    
    sec_code = row_data.get('è‚¡ç¥¨ä»£ç ', row_data.get('è¯åˆ¸ä»£ç ', 'æœªçŸ¥ä»£ç '))
    sec_name = row_data.get('å…¬å¸åç§°', 'æœªçŸ¥å…¬å¸')
    report_title = row_data.get('è´¢æŠ¥åç§°', 'æœªçŸ¥æŠ¥å‘Š')
    report_date = row_data.get('æŠ¥å‘Šæ—¥æœŸ', 'æœªçŸ¥æ—¥æœŸ')
    
    # æ¸…ç†å¹¶æ„é€ æ–‡ä»¶å
    report_name_base = f"{sec_name}ï¼š{report_title}_[{report_date}]"
    file_name = re.sub(r'[\\/:*?"<>|]', '_', report_name_base) + ".pdf"
    file_path = os.path.join(folder_path, file_name)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆä»…åœ¨ä¸‹è½½PDFæ¨¡å¼ä¸‹æ£€æŸ¥ï¼‰
    if download_pdf and os.path.exists(file_path):
        print(f"  ğŸ“„ æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¯»å–: {file_name}")
        try:
            with open(file_path, 'rb') as f:
                pdf_content = f.read()
        except Exception as e:
            print(f"  âŒ è¯»å–å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥: {e}")
            return []
    else:
        try:
            print(f"  ğŸ“¥ æ­£åœ¨ä¸‹è½½: {sec_name} - {report_title}")
            response = session.get(pdf_url, headers=headers, timeout=(15, 45))
            response.raise_for_status()
            
            # éªŒè¯æ˜¯å¦ä¸ºPDF
            if 'application/pdf' not in response.headers.get('Content-Type', ''):
                print(f"  âš ï¸ è­¦å‘Š: {file_name} ä¸æ˜¯PDFæ–‡ä»¶ã€‚")
                return []

            pdf_content = response.content

            # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šæ˜¯å¦ä¿å­˜PDFæ–‡ä»¶åˆ°æœ¬åœ°
            if download_pdf:
                os.makedirs(folder_path, exist_ok=True)
                with open(file_path, 'wb') as f:
                    f.write(pdf_content)
                print(f"  âœ… PDFå·²ä¿å­˜: {file_name}")
            else:
                print(f"  ğŸ“Š ä»…è§£ææ•°æ®ï¼Œæœªä¿å­˜PDF: {file_name}")
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
            return []

    # åœ¨å†…å­˜ä¸­è§£æPDFå†…å®¹ - ä»…ä½¿ç”¨è¡¨æ ¼æå–é€»è¾‘
    print(f"  ğŸ” ä½¿ç”¨è¡¨æ ¼æå–æ–¹æ³•...")
    extracted_data_table, has_data_resource_keyword = extract_data_by_table(pdf_content, pdf_url)
    
    all_extracted_data = extracted_data_table
    print(f"  ğŸ“Š è¡¨æ ¼æ–¹æ³•æ‰¾åˆ°: {len(extracted_data_table)} æ¡")
    
    # å°†æŠ¥å‘Šè‡ªèº«ä¿¡æ¯æ·»åŠ åˆ°æå–ç»“æœä¸­
    results_for_excel = []
    if all_extracted_data:
        for item in all_extracted_data:
            adjusted_amount = adjust_amount_for_special_unit(sec_code, item['value'])
            results_for_excel.append({
                "è¯åˆ¸ä»£ç ": sec_code,
                "å…¬å¸åç§°": sec_name,
                "æŠ¥å‘Šåç§°": report_title,
                "æŠ¥å‘Šæ—¥æœŸ": report_date,
                "é¡¹ç›®åç§°": item['category'],
                "é‡‘é¢": adjusted_amount,
                "PDFé“¾æ¥": pdf_url,
                "_has_data_resource": 1 if has_data_resource_keyword else 0  # ä¸´æ—¶å­—æ®µï¼Œç”¨äºåç»­åˆ¤æ–­
            })
    else:
        # å³ä½¿æ²¡æ‰¾åˆ°æ•°æ®ï¼Œä¹Ÿè®°å½•ä¸‰æ¡ï¼ˆå¯¹åº”ä¸‰ä¸ªé¡¹ç›®ï¼‰ï¼Œæ–¹ä¾¿è¿½æº¯ï¼Œé‡‘é¢è®¾ä¸º0
        for category in ["å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"]:
            adjusted_amount = adjust_amount_for_special_unit(sec_code, "0")
            results_for_excel.append({
                "è¯åˆ¸ä»£ç ": sec_code,
                "å…¬å¸åç§°": sec_name,
                "æŠ¥å‘Šåç§°": report_title,
                "æŠ¥å‘Šæ—¥æœŸ": report_date,
                "é¡¹ç›®åç§°": category,
                "é‡‘é¢": adjusted_amount,
                "PDFé“¾æ¥": pdf_url,
                "_has_data_resource": 1 if has_data_resource_keyword else 0  # ä¸´æ—¶å­—æ®µï¼Œç”¨äºåç»­åˆ¤æ–­
            })
            
    return results_for_excel


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="æŠ¥å‘Šä¿¡æ¯æ”¶é›†å™¨ - ä»CSVæ–‡ä»¶è¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®èµ„æºä¿¡æ¯"
    )
    parser.add_argument(
        "--csv-file",
        type=str,
        default=None,
        help="æŒ‡å®šCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„listed_companies_*.csvæ–‡ä»¶"
    )
    parser.add_argument(
        "--no-download",
        action="store_true",
        help="ä¸ä¸‹è½½PDFæ–‡ä»¶ï¼Œä»…è§£ææ•°æ®ç”ŸæˆExcelï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šæ­¤å‚æ•°ï¼Œç¨‹åºä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½"
    )
    parser.add_argument(
        "--download-pdf",
        action="store_true",
        help="ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå®Œæ•´æ¨¡å¼ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šæ­¤å‚æ•°ï¼Œç¨‹åºä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½"
    )
    return parser.parse_args()


def find_csv_file(csv_file_path=None):
    """
    æŸ¥æ‰¾ç¬¦åˆå‘½åæ¨¡å¼çš„CSVæ–‡ä»¶å¹¶è§£ææ–‡ä»¶åä¿¡æ¯
    
    Args:
        csv_file_path (str, optional): æŒ‡å®šçš„CSVæ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæä¾›ï¼Œç›´æ¥ä½¿ç”¨è¯¥æ–‡ä»¶ï¼›å¦åˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
    
    Returns:
        tuple: (CSVæ–‡ä»¶è·¯å¾„, è§£æä¿¡æ¯å­—å…¸) æˆ– (None, None)
    """
    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
    if csv_file_path:
        if not os.path.exists(csv_file_path):
            print(f"âŒ æŒ‡å®šçš„CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
            return None, None
        print(f"ğŸ“„ ä½¿ç”¨æŒ‡å®šçš„CSVæ–‡ä»¶: {csv_file_path}")
        latest_file = csv_file_path
    else:
        # æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„CSVæ–‡ä»¶
        pattern = "listed_companies_*_*.csv"
        csv_files = glob.glob(pattern)
        
        if not csv_files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆå‘½åæ¨¡å¼çš„CSVæ–‡ä»¶ï¼ˆlisted_companies_*_*.csvï¼‰")
            return None, None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°çš„ï¼ˆæ›´å‡†ç¡®ï¼‰
        def extract_timestamp_from_filename(filename):
            """ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³ç”¨äºæ’åº"""
            try:
                name_without_ext = os.path.basename(filename).replace('.csv', '')
                parts = name_without_ext.split('_')
                # æ–‡ä»¶åæ ¼å¼ï¼šlisted_companies_{start_date}_{end_date}_{report_type}_{timestamp}
                # timestamp æ ¼å¼é€šå¸¸æ˜¯ YYYYMMDD_HHMMSS
                if len(parts) >= 6:
                    timestamp_str = '_'.join(parts[5:])  # è·å–æ—¶é—´æˆ³éƒ¨åˆ†
                    # å°è¯•è§£ææ—¶é—´æˆ³
                    if '_' in timestamp_str:
                        date_part, time_part = timestamp_str.split('_', 1)
                        # è½¬æ¢ä¸ºå¯æ¯”è¾ƒçš„æ ¼å¼
                        return f"{date_part}_{time_part}"
                # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºå¤‡é€‰
                return str(os.path.getmtime(filename))
            except:
                return str(os.path.getmtime(filename))
        
        csv_files.sort(key=extract_timestamp_from_filename, reverse=True)
        latest_file = csv_files[0]
        
        print(f"ğŸ“„ æ‰¾åˆ°CSVæ–‡ä»¶: {latest_file}")
        if len(csv_files) > 1:
            print(f"   æç¤º: æ‰¾åˆ° {len(csv_files)} ä¸ªåŒ¹é…æ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°çš„ï¼ˆæŒ‰æ–‡ä»¶åæ—¶é—´æˆ³ï¼‰: {latest_file}")
            print(f"   å…¶ä»–æ–‡ä»¶: {', '.join(csv_files[1:3])}..." if len(csv_files) > 3 else f"   å…¶ä»–æ–‡ä»¶: {', '.join(csv_files[1:])}")
    
    # è§£ææ–‡ä»¶åï¼šlisted_companies_{start_date}_{end_date}_{report_type}_{timestamp}.csv
    file_name = os.path.basename(latest_file)
    # å»æ‰æ‰©å±•å
    name_without_ext = file_name.replace('.csv', '')
    # åˆ†å‰²æ–‡ä»¶å
    parts = name_without_ext.split('_')
    
    if len(parts) >= 5:
        # listed_companies_{start_date}_{end_date}_{report_type}_{timestamp}
        start_date_str = parts[2]  # ä¾‹å¦‚: 20250801
        end_date_str = parts[3]    # ä¾‹å¦‚: 20250831
        report_type = parts[4]     # ä¾‹å¦‚: bndbg
        # timestamp å¯èƒ½åŒ…å«ä¸‹åˆ’çº¿ï¼Œæ‰€ä»¥å–å‰©ä½™éƒ¨åˆ†
        timestamp = '_'.join(parts[5:]) if len(parts) > 5 else ''
        
        file_info = {
            'start_date_str': start_date_str,
            'end_date_str': end_date_str,
            'report_type': report_type,
            'original_timestamp': timestamp
        }
        
        print(f"ğŸ“‹ è§£ææ–‡ä»¶åä¿¡æ¯:")
        print(f"   å¼€å§‹æ—¥æœŸ: {start_date_str}")
        print(f"   ç»“æŸæ—¥æœŸ: {end_date_str}")
        print(f"   æŠ¥å‘Šç±»å‹: {report_type}")
        
        return latest_file, file_info
    else:
        print(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶åæ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å‘½å")
        return latest_file, None


def pivot_to_wide_format(df_long):
    """
    å°†é•¿æ ¼å¼æ•°æ®è½¬æ¢ä¸ºå®½æ ¼å¼ï¼Œå¹¶æ·»åŠ "æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"åˆ—
    å»é‡é€»è¾‘ï¼š
    1. åŒä¸€ä¸ªè¯åˆ¸ä»£ç ä¸‹çš„å­˜è´§/æ— å½¢èµ„äº§/å¼€å‘æ”¯å‡ºï¼Œä¼˜å…ˆå–å¤§äº0çš„å€¼
    2. å¦‚æœæœ‰å¤šä¸ªå€¼ä¸”ç›¸ç­‰ï¼Œå–ç¬¬ä¸€ä¸ª
    3. å¦‚æœæœ‰å¤šä¸ªå€¼ä¸”ä¸ç›¸ç­‰ï¼Œå–ç¬¬ä¸€ä¸ªå¤§äº0çš„ï¼ˆå¦‚æœéƒ½æ²¡æœ‰å¤§äº0çš„ï¼Œå–ç¬¬ä¸€ä¸ªï¼‰
    
    Args:
        df_long (pd.DataFrame): é•¿æ ¼å¼æ•°æ®
    
    Returns:
        pd.DataFrame: å®½æ ¼å¼æ•°æ®
    """
    print("\næ­£åœ¨è¿›è¡Œæ•°æ®é€è§†æ“ä½œ...")
    
    # å»é‡é€»è¾‘ï¼šæŒ‰è¯åˆ¸ä»£ç å’Œé¡¹ç›®åç§°åˆ†ç»„ï¼Œä¼˜å…ˆé€‰æ‹©å¤§äº0çš„å€¼
    print("æ­£åœ¨æŒ‰è§„åˆ™å»é‡...")
    
    def get_numeric_value(val):
        """å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ï¼Œç”¨äºæ¯”è¾ƒ"""
        if pd.isna(val):
            return 0
        try:
            val_str = str(val).strip().replace(',', '').replace(' ', '')
            if val_str in ['N/A', 'ç©ºå€¼', '-', 'nan', 'None', '0', '']:
                return 0
            return float(val_str)
        except:
            return 0
    
    # æŒ‰è¯åˆ¸ä»£ç ã€å…¬å¸åç§°ã€æŠ¥å‘Šåç§°ã€æŠ¥å‘Šæ—¥æœŸã€PDFé“¾æ¥ã€é¡¹ç›®åç§°åˆ†ç»„
    deduplicated_rows = []
    
    grouped = df_long.groupby(['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ', 'PDFé“¾æ¥', 'é¡¹ç›®åç§°'])
    
    for (sec_code, company, report, date, pdf_link, category), group in grouped:
        if len(group) == 1:
            # åªæœ‰ä¸€æ¡è®°å½•ï¼Œç›´æ¥æ·»åŠ 
            deduplicated_rows.append(group.iloc[0].to_dict())
        else:
            # å¤šæ¡è®°å½•ï¼ŒæŒ‰è§„åˆ™é€‰æ‹©
            group = group.copy()
            group['_numeric_value'] = group['é‡‘é¢'].apply(get_numeric_value)
            
            # ä¼˜å…ˆé€‰æ‹©å¤§äº0çš„å€¼
            positive_rows = group[group['_numeric_value'] > 0]
            
            if len(positive_rows) > 0:
                # å¦‚æœæœ‰å¤§äº0çš„å€¼ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                selected_row = positive_rows.iloc[0]
            else:
                # å¦‚æœæ²¡æœ‰å¤§äº0çš„å€¼ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                selected_row = group.iloc[0]
            
            deduplicated_rows.append(selected_row.drop('_numeric_value').to_dict())
    
    df_long_dedup = pd.DataFrame(deduplicated_rows)
    print(f"å»é‡å‰: {len(df_long)} è¡Œï¼Œå»é‡å: {len(df_long_dedup)} è¡Œ")
    
    # åˆ›å»ºé‡‘é¢é€è§†è¡¨
    df_pivot = df_long_dedup.pivot_table(
        index=['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ', 'PDFé“¾æ¥'], 
        columns='é¡¹ç›®åç§°',                           
        values='é‡‘é¢',                                
        aggfunc='first'                               
    ).reset_index()
    
    print("æ•°æ®é€è§†å®Œæˆï¼")
    
    # åˆ›å»º"æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"åˆ—
    # æ–°é€»è¾‘ï¼šåªè¦PDFä¸­æœ‰"æ•°æ®èµ„æº"è¿™ä¸ªè¯ï¼Œå°±è®¾ä¸º1
    # ä»åŸå§‹é•¿æ ¼å¼æ•°æ®ä¸­è·å–æ¯ä¸ªPDFçš„æ ‡è®°
    has_data_col = []
    item_cols = ['å­˜è´§', 'æ— å½¢èµ„äº§', 'å¼€å‘æ”¯å‡º']  # å®šä¹‰é¡¹ç›®åˆ—ï¼Œç”¨äºåç»­æ•°æ®æ¸…ç†
    
    # ä¸ºæ¯ä¸ªPDFé“¾æ¥åˆ›å»ºä¸€ä¸ªæ ‡è®°å­—å…¸
    pdf_has_data_resource = {}
    if '_has_data_resource' in df_long_dedup.columns:
        for pdf_link in df_long_dedup['PDFé“¾æ¥'].unique():
            pdf_rows = df_long_dedup[df_long_dedup['PDFé“¾æ¥'] == pdf_link]
            if len(pdf_rows) > 0:
                # å–ç¬¬ä¸€æ¡è®°å½•çš„æ ‡è®°ï¼ˆæ‰€æœ‰è®°å½•çš„æ ‡è®°åº”è¯¥ç›¸åŒï¼‰
                pdf_has_data_resource[pdf_link] = int(pdf_rows.iloc[0]['_has_data_resource'])
            else:
                pdf_has_data_resource[pdf_link] = 0
    else:
        # å¦‚æœä¸´æ—¶å­—æ®µä¸å­˜åœ¨ï¼Œé»˜è®¤éƒ½æ˜¯0ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
        for pdf_link in df_pivot['PDFé“¾æ¥'].unique():
            pdf_has_data_resource[pdf_link] = 0
    
    # æ ¹æ®PDFé“¾æ¥è®¾ç½®"æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"
    for idx, row in df_pivot.iterrows():
        pdf_link = row['PDFé“¾æ¥']
        has_data = pdf_has_data_resource.get(pdf_link, 0)
        has_data_col.append(has_data)
    
    df_pivot['æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§'] = has_data_col
    
    # å°†æ‰€æœ‰ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0
    print("æ­£åœ¨æ¸…ç†æ•°æ®ï¼šå°†ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0...")
    for col in item_cols:
        if col in df_pivot.columns:
            df_pivot[col] = df_pivot[col].replace(['N/A', 'ç©ºå€¼', '-', 'nan', 'None', ''], '0')
            df_pivot[col] = df_pivot[col].fillna('0')
    
    # è°ƒæ•´åˆ—é¡ºåºï¼šåŸºæœ¬ä¿¡æ¯ -> é‡‘é¢åˆ— -> æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§ -> PDFé“¾æ¥
    base_cols = ['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ']
    amount_cols = [col for col in item_cols if col in df_pivot.columns]
    other_cols = ['æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§', 'PDFé“¾æ¥']
    
    final_columns = base_cols + amount_cols + other_cols
    # åªä¿ç•™å­˜åœ¨çš„åˆ—
    final_columns = [col for col in final_columns if col in df_pivot.columns]
    
    df_final = df_pivot[final_columns]
    
    return df_final


def main():
    """
    ä¸»å‡½æ•° - ä»CSVè¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šæ˜¯å¦ä¸‹è½½PDF
    if args.no_download:
        download_pdf = False
        print("\nâœ… å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ï¼šä»…ç”ŸæˆExcelæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼Œä¸ä¸‹è½½PDFï¼‰")
    elif args.download_pdf:
        download_pdf = True
        print("\nâœ… å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ï¼šä¸‹è½½PDFå¹¶ç”ŸæˆExcelï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
    else:
        # è¯¢é—®æ˜¯å¦ä¸‹è½½PDF
        print("\n" + "="*60)
        print("æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°ï¼Ÿ")
        print("y - ä¸‹è½½PDFå¹¶ç”ŸæˆExcelï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
        print("n - ä»…ç”ŸæˆExcelæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼Œä¸ä¸‹è½½PDFï¼‰")
        print("="*60)
        
        while True:
            choice = input("è¯·è¾“å…¥é€‰æ‹© (y/n): ").strip().lower()
            if choice == 'y':
                download_pdf = True
                print("âœ… å·²é€‰æ‹©ï¼šä¸‹è½½PDFå¹¶ç”ŸæˆExcelï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
                break
            elif choice == 'n':
                download_pdf = False
                print("âœ… å·²é€‰æ‹©ï¼šä»…ç”ŸæˆExcelæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ y æˆ– n")
    
    print(f"\nğŸ“ PDFä¸‹è½½æ¨¡å¼: {'å¼€å¯' if download_pdf else 'å…³é—­'}")
    if not download_pdf:
        print("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šä»…è§£æPDFå†…å®¹ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°")
    else:
        print("ğŸ’¾ å®Œæ•´æ¨¡å¼ï¼šä¸‹è½½å¹¶ä¿å­˜PDFæ–‡ä»¶åˆ°æœ¬åœ°")
    
    # æŸ¥æ‰¾CSVæ–‡ä»¶
    csv_file, file_info = find_csv_file(args.csv_file)
    if not csv_file:
        return
    
    # è¯»å–CSVæ–‡ä»¶
    try:
        print(f"\nğŸ“– æ­£åœ¨è¯»å–CSVæ–‡ä»¶: {csv_file}")
        df_csv = pd.read_csv(csv_file, dtype=str)
        print(f"âœ… æˆåŠŸè¯»å– {len(df_csv)} æ¡è®°å½•")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['PDFé“¾æ¥']
        missing_cols = [col for col in required_cols if col not in df_csv.columns]
        if missing_cols:
            print(f"âŒ CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
            return
        
        # æ˜¾ç¤ºåˆ—å
        print(f"ğŸ“‹ CSVæ–‡ä»¶åŒ…å«çš„åˆ—: {', '.join(df_csv.columns.tolist())}")
        
    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # åˆå§‹åŒ–
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Accept": "application/pdf, application/json, text/plain, */*",
        "Referer": "http://www.cninfo.com.cn/new/commonUrl?url=disclosure/list/notice",
    }
    
    folder_path = os.path.join(os.getcwd(), "FinancialReports_Collection")
    all_results_for_excel = []
    start_time = time.time()
    
    print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(df_csv)} ä¸ªPDFé“¾æ¥...")
    print("="*60)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_row = {
            executor.submit(process_pdf_link, row.to_dict(), session, headers, folder_path, download_pdf): idx 
            for idx, row in df_csv.iterrows()
        }
        
        completed = 0
        for future in as_completed(future_to_row):
            completed += 1
            try:
                extracted_data = future.result()
                if extracted_data:
                    all_results_for_excel.extend(extracted_data)
                print(f"ğŸ“Š è¿›åº¦: {completed}/{len(df_csv)} ({completed/len(df_csv)*100:.1f}%)")
            except Exception as exc:
                print(f'âŒ ä¸€ä¸ªä»»åŠ¡åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {exc}')
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ¯ å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"  ğŸ“Š æ€»è®°å½•æ•°: {len(df_csv)}")
    # ç»Ÿè®¡æˆåŠŸæå–çš„æ•°æ®ï¼ˆé‡‘é¢å¤§äº0çš„è®°å½•æ•°ï¼‰
    success_count = 0
    for r in all_results_for_excel:
        amount = r.get('é‡‘é¢', '0')
        if amount and str(amount) not in ['0', 'N/A', 'ç©ºå€¼', '-', 'nan', 'None', '']:
            try:
                if float(str(amount).replace(',', '')) > 0:
                    success_count += 1
            except:
                pass
    print(f"  âœ… æˆåŠŸæå–æ•°æ®ï¼ˆé‡‘é¢>0ï¼‰: {success_count}")
    
    # ç”Ÿæˆæœ€ç»ˆçš„ExcelæŠ¥å‘Š
    print("\n===== æ­£åœ¨ç”ŸæˆExcelæŠ¥å‘Š... =====")
    if all_results_for_excel:
        # ç”Ÿæˆé•¿æ ¼å¼æŠ¥å‘Š
        df_long = pd.DataFrame(all_results_for_excel)
        # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—ï¼ˆä¸å»é‡ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®ï¼‰
        # æ³¨æ„ï¼šå¿…é¡»ä¿ç•™_has_data_resourceå­—æ®µï¼Œä¾›pivot_to_wide_formatä½¿ç”¨
        required_cols = ['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ', 'é¡¹ç›®åç§°', 'é‡‘é¢', 'PDFé“¾æ¥']
        if '_has_data_resource' in df_long.columns:
            required_cols.append('_has_data_resource')
        available_cols = [col for col in required_cols if col in df_long.columns]
        df_long = df_long[available_cols]
        
        # å°†æ‰€æœ‰ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0ï¼ˆé•¿æ ¼å¼ä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œä¸å»é‡ï¼‰
        print("æ­£åœ¨æ¸…ç†æ•°æ®ï¼šå°†ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0...")
        df_long['é‡‘é¢'] = df_long['é‡‘é¢'].replace(['N/A', 'ç©ºå€¼', '-', 'nan', 'None', ''], '0')
        df_long['é‡‘é¢'] = df_long['é‡‘é¢'].fillna('0')
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºtimestamp
        output_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if file_info:
            # ä»CSVæ–‡ä»¶åä¸­æå–çš„ä¿¡æ¯
            start_date_str = file_info['start_date_str']
            end_date_str = file_info['end_date_str']
            report_type = file_info['report_type']
            
            # æ„å»ºæ–‡ä»¶åï¼šlong_output_{start_date}_{end_date}_{report_type}_{timestamp}.xlsx
            long_output_filename = f'long_output_{start_date_str}_{end_date_str}_{report_type}_{output_timestamp}.xlsx'
            wide_output_filename = f'wide_output_{start_date_str}_{end_date_str}_{report_type}_{output_timestamp}.xlsx'
        else:
            # å¦‚æœæ— æ³•è§£ææ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤å‘½å
            long_output_filename = f'long_output_{output_timestamp}.xlsx'
            wide_output_filename = f'wide_output_{output_timestamp}.xlsx'
        
        # ç”Ÿæˆé•¿æ ¼å¼Excel
        if os.path.exists(long_output_filename):
            os.remove(long_output_filename)
        df_long.to_excel(long_output_filename, index=False)
        print(f"ğŸ‰ é•¿æ ¼å¼æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜ä¸º ./{long_output_filename}")
        
        # ç”Ÿæˆå®½æ ¼å¼æŠ¥å‘Š
        print("\næ­£åœ¨ç”Ÿæˆå®½æ ¼å¼æŠ¥å‘Š...")
        df_wide = pivot_to_wide_format(df_long)
        
        if os.path.exists(wide_output_filename):
            os.remove(wide_output_filename)
        
        df_wide.to_excel(wide_output_filename, index=False, freeze_panes=(1, 0))
        print(f"ğŸ‰ å®½æ ¼å¼æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜ä¸º ./{wide_output_filename}")
        
        print("\nğŸ“Š å®½æ ¼å¼æŠ¥å‘Šé¢„è§ˆ:")
        print(df_wide.head(10))
    else:
        print("âŒ æœªæå–åˆ°ä»»ä½•æ•°æ®ï¼Œä¸ç”ŸæˆExcelæ–‡ä»¶ã€‚")
        
    end_time = time.time()
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {(end_time - start_time):.2f} ç§’")
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    print("=" * 60)
    print("æŠ¥å‘Šä¿¡æ¯æ”¶é›†å™¨ - æ•°æ®èµ„æºæå–å·¥å…·")
    print("=" * 60)
    print("åŠŸèƒ½ï¼šä»CSVæ–‡ä»¶è¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®èµ„æºä¿¡æ¯")
    print("è¾“å‡ºï¼šç”Ÿæˆé•¿æ ¼å¼å’Œå®½æ ¼å¼çš„ExcelæŠ¥å‘Š")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•ï¼š")
    print("  python report_info_collection.py                              # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„CSVæ–‡ä»¶ï¼Œä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½PDF")
    print("  python report_info_collection.py --csv-file file.csv          # æŒ‡å®šCSVæ–‡ä»¶ï¼Œä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½PDF")
    print("  python report_info_collection.py --no-download                 # ä¸ä¸‹è½½PDFï¼Œä»…è§£ææ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
    print("  python report_info_collection.py --download-pdf                # ä¸‹è½½PDFåˆ°æœ¬åœ°ï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
    print("  python report_info_collection.py --csv-file file.csv --no-download  # æŒ‡å®šCSVæ–‡ä»¶ä¸”ä¸ä¸‹è½½PDF")
    print("=" * 60)
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nç¨‹åºæ‰§è¡Œå®Œæ¯•")

