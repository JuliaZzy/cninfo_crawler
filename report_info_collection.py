#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ¥å‘Šä¿¡æ¯æ”¶é›†å™¨ - ä»CSVæ–‡ä»¶è¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®èµ„æºä¿¡æ¯

åŠŸèƒ½ï¼š
1. ä»CSVæ–‡ä»¶ä¸­è¯»å–PDFé“¾æ¥
2. é€ä¸ªè§£æPDFï¼Œæå–"å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"çš„æ•°æ®
3. ç”Ÿæˆé•¿æ ¼å¼å’Œå®½æ ¼å¼çš„ExcelæŠ¥å‘Š
4. æ·»åŠ "æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"æ ‡è®°åˆ—

ä½œè€…ï¼šåŸºäºfinancial_data_crawler.pyè½¬æ¢
æ—¥æœŸï¼š2025å¹´
"""

import os
import re
import requests
import pandas as pd
import pdfplumber
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from datetime import datetime
import warnings
import logging
from pathlib import Path
import glob
import argparse

# æŠ‘åˆ¶pdfplumberçš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")
logging.getLogger("pdfplumber").setLevel(logging.ERROR)


def extract_data_by_text(pdf_content, pdf_url):
    """
    æ–¹æ³•1ï¼šé€šè¿‡æ–‡æœ¬æœç´¢æŸ¥æ‰¾"å…¶ä¸­ï¼šæ•°æ®èµ„æº"ï¼Œä¸ä¾èµ–è¡¨æ ¼æå–ã€‚
    æ‰¾åˆ°åæ£€æŸ¥ä¸Šä¸€è¡Œçš„çˆ¶ç±»åˆ«ï¼Œå¹¶æå–æ•°å€¼ã€‚
    åŒæ—¶æ£€æŸ¥PDFä¸­æ˜¯å¦åŒ…å«"æ•°æ®èµ„æº"è¿™ä¸ªè¯ã€‚
    
    Args:
        pdf_content (bytes): PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
        pdf_url (str): PDFæ–‡ä»¶çš„URLï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    Returns:
        tuple: (åŒ…å«æå–æ•°æ®çš„å­—å…¸åˆ—è¡¨, æ˜¯å¦åŒ…å«"æ•°æ®èµ„æº"å…³é”®è¯)
    """
    found_items = []
    parent_categories = ["å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"]
    has_data_resource_keyword = False  # æ ‡è®°æ˜¯å¦åœ¨PDFä¸­æ‰¾åˆ°"æ•°æ®èµ„æº"è¿™ä¸ªè¯
    
    def extract_number_from_text(text):
        """
        ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°å­—ï¼ˆä¿ç•™åƒåˆ†ä½æ ¼å¼ï¼‰
        
        Args:
            text (str): æ–‡æœ¬å†…å®¹
            
        Returns:
            tuple: (æ‰¾åˆ°çš„æ•°å­—å­—ç¬¦ä¸², æ˜¯å¦æ£€æµ‹åˆ°æ•°å­—, æ•°å€¼æ˜¯å¦å¤§äº0)
        """
        if not text:
            return "ç©ºå€¼", False, False
        
        # æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™é€—å·ï¼ˆåƒåˆ†ä½ï¼‰
        cleaned_text = text.strip().replace(' ', '')
        
        # æ•°å­—åŒ¹é…æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œæ›´ç²¾ç¡®çš„åœ¨å‰ï¼‰
        number_patterns = [
            r'((?:\d{1,3},)*\d{1,3}\.\d{2})',  # æ ‡å‡†æ ¼å¼ï¼š1,234.56
            r'((?:\d{1,3},)*\d{1,3}\.\d+)',    # å¸¦å°æ•°ç‚¹çš„æ ¼å¼ï¼š1,234.5 æˆ– 1,234.567
            r'((?:\d{1,3},)+\d+)',             # å¸¦åƒåˆ†ä½çš„æ•´æ•°ï¼š1,234,567
            r'((?:\d{1,3},)*\d+)',              # æ•´æ•°æ ¼å¼ï¼š1,234
            r'(\d+\.\d{2})',                    # ç®€å•å°æ•°ï¼š123.45
            r'(\d+\.\d+)',                      # å¸¦å°æ•°ç‚¹çš„æ•°å­—ï¼š123.5
            r'(\d+)',                           # çº¯æ•°å­—ï¼š123ï¼ˆä»»ä½•ä½æ•°ï¼‰
        ]
        
        for pattern in number_patterns:
            match = re.search(pattern, cleaned_text)
            if match:
                value_str = match.group(1)
                # è½¬æ¢ä¸ºæ•°å€¼æ£€æŸ¥æ˜¯å¦å¤§äº0
                try:
                    # å»é™¤é€—å·åè½¬æ¢ä¸ºæµ®ç‚¹æ•°
                    numeric_value = float(value_str.replace(',', ''))
                    is_positive = numeric_value > 0
                    return value_str, True, is_positive
                except:
                    return value_str, True, True  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå‡è®¾å¤§äº0
        
        return "ç©ºå€¼", False, False
    
    try:
        # ä¸´æ—¶æŠ‘åˆ¶pdfplumberçš„è­¦å‘Šå’Œé”™è¯¯è¾“å‡º
        import sys
        from io import StringIO
        
        # æ•è·stderrä»¥æŠ‘åˆ¶pdfplumberçš„è­¦å‘Š
        old_stderr = sys.stderr
        sys.stderr = StringIO()
        
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with pdfplumber.open(BytesIO(pdf_content)) as pdf:
                    for page_num, page in enumerate(pdf.pages, 1):
                        # å…ˆæ£€æŸ¥æ•´ä¸ªé¡µé¢æ˜¯å¦åŒ…å«"æ•°æ®èµ„æº"ï¼ˆç”¨äº"æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"æ ‡è®°ï¼‰
                        page_text = page.extract_text() or ""
                        if "æ•°æ®èµ„æº" in page_text:
                            has_data_resource_keyword = True
                        
                        # æå–æ‰€æœ‰å•è¯ï¼ˆå¸¦ä½ç½®ä¿¡æ¯ï¼‰
                        words = page.extract_words()
                        if not words:
                            continue
                        
                        # æŒ‰è¡Œç»„ç»‡å•è¯ï¼ˆé€šè¿‡yåæ ‡åˆ†ç»„ï¼‰
                        # å°†yåæ ‡ç›¸è¿‘çš„å•è¯å½’ä¸ºåŒä¸€è¡Œ
                        lines = {}
                        for word in words:
                            # ä½¿ç”¨yåæ ‡çš„æ•´æ•°éƒ¨åˆ†ä½œä¸ºè¡Œæ ‡è¯†
                            y_key = round(word['top'])
                            if y_key not in lines:
                                lines[y_key] = []
                            lines[y_key].append(word)
                        
                        # æŒ‰yåæ ‡ä»å¤§åˆ°å°æ’åºï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰
                        sorted_lines = sorted(lines.items(), key=lambda x: x[0], reverse=True)
                        
                        # æŸ¥æ‰¾åŒ…å«"å…¶ä¸­ï¼šæ•°æ®èµ„æº"çš„è¡Œï¼ˆä½¿ç”¨æ­£åˆ™åŒ¹é…ï¼Œå…è®¸å†’å·å˜ä½“å’Œç©ºæ ¼ï¼‰
                        # åŒ¹é…æ¨¡å¼ï¼šå…¶ä¸­ + å†’å·ï¼ˆä¸­æ–‡/è‹±æ–‡/å…¨è§’ï¼‰ + å¯é€‰ç©ºæ ¼ + æ•°æ®èµ„æº
                        target_pattern = re.compile(r'å…¶ä¸­[ï¼š:ï¼š]\s*æ•°æ®èµ„æº')
                        
                        for line_idx, (y_pos, line_words) in enumerate(sorted_lines):
                            # æ£€æŸ¥è¿™ä¸€è¡Œæ˜¯å¦åŒ…å«ç›®æ ‡æ–‡æœ¬ï¼ˆå…ˆæ‹¼æ¥å®Œæ•´è¡Œæ–‡æœ¬ï¼Œä¹Ÿæ£€æŸ¥å•ä¸ªå•è¯çš„ç»„åˆï¼‰
                            line_text = ' '.join([w['text'] for w in line_words])
                            
                            # ä¹Ÿæ£€æŸ¥å»é™¤ç©ºæ ¼åçš„æ–‡æœ¬ï¼ˆé˜²æ­¢ç©ºæ ¼å¹²æ‰°ï¼‰
                            line_text_no_space = line_text.replace(' ', '').replace('ã€€', '')  # å»é™¤æ™®é€šç©ºæ ¼å’Œå…¨è§’ç©ºæ ¼
                            
                            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…"å…¶ä¸­ï¼šæ•°æ®èµ„æº"ï¼ˆå…è®¸å†’å·å˜ä½“å’Œç©ºæ ¼ï¼‰
                            if target_pattern.search(line_text) or target_pattern.search(line_text_no_space):
                                # åœ¨è¿™ä¸€è¡Œä¸­æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¤§äº0çš„æ•°å€¼
                                found_value = "ç©ºå€¼"
                                has_number = False
                                found_zero_value = False
                                
                                # åœ¨åŒä¸€è¡Œçš„æ‰€æœ‰å•è¯ä¸­æŸ¥æ‰¾æ•°å€¼
                                for word in line_words:
                                    value, has_num, is_positive = extract_number_from_text(word['text'])
                                    if has_num and value != "ç©ºå€¼":
                                        if is_positive:
                                            found_value = value
                                            has_number = True
                                            break
                                        else:
                                            # æ‰¾åˆ°äº†æ•°å€¼ä½†æ˜¯ä¸º0
                                            found_zero_value = True
                                
                                # å¦‚æœæ‰¾åˆ°æ•°å€¼ä¸º0ï¼Œè·³è¿‡
                                if found_zero_value and not has_number:
                                    continue
                                
                                # å¦‚æœæ‰¾åˆ°æ•°å€¼ä¸”æ•°å€¼å¤§äº0ï¼Œå‘ä¸ŠæŸ¥æ‰¾çˆ¶ç±»åˆ«
                                if has_number and found_value != "ç©ºå€¼":
                                    parent_category = None
                                    
                                    # å‘ä¸ŠæŸ¥æ‰¾çˆ¶ç±»åˆ«ï¼ˆæ£€æŸ¥ä¸Šé¢çš„è¡Œï¼‰
                                    # sorted_linesæ˜¯æŒ‰yä»å¤§åˆ°å°æ’åºï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ï¼Œæ‰€ä»¥ä¸Šä¸€è¡Œæ˜¯line_idx-1
                                    # å‘ä¸ŠæŸ¥æ‰¾æœ€å¤š2è¡Œ
                                    for prev_line_idx in range(max(0, line_idx - 2), line_idx):
                                        prev_y_pos, prev_line_words = sorted_lines[prev_line_idx]
                                        prev_line_text = ' '.join([w['text'] for w in prev_line_words])
                                        
                                        for cat in parent_categories:
                                            if cat in prev_line_text:
                                                parent_category = cat
                                                break
                                        
                                        if parent_category:
                                            break
                                    
                                    # å¦‚æœæ‰¾åˆ°çˆ¶ç±»åˆ«ï¼Œæ·»åŠ åˆ°ç»“æœ
                                    if parent_category:
                                        found_items.append({
                                            "category": parent_category,
                                            "value": found_value,
                                            "method": "text"  # æ ‡è®°æ¥æº
                                        })
                                        print(f"    âœ… [æ–‡æœ¬] ç¬¬{page_num}é¡µ {parent_category}å…¶ä¸­ï¼šæ•°æ®èµ„æº: {found_value}")
        finally:
            # æ¢å¤stderr
            sys.stderr = old_stderr
            
    except Exception as e:
        print(f"    âŒ è§£æPDFæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return []
        
    if not found_items:
        print(f"    âš ï¸ åœ¨æ­¤PDFä¸­æœªæ‰¾åˆ°'å…¶ä¸­ï¼šæ•°æ®èµ„æº'ç›¸å…³æ¡ç›®ã€‚")
        
    return found_items, has_data_resource_keyword


def extract_data_by_table(pdf_content, pdf_url):
    """
    æ–¹æ³•2ï¼šé€šè¿‡è¡¨æ ¼æå–æŸ¥æ‰¾"å…¶ä¸­ï¼šæ•°æ®èµ„æº"ã€‚
    æ‰¾åˆ°åæ£€æŸ¥ä¸Šä¸€è¡Œçš„çˆ¶ç±»åˆ«ï¼Œå¹¶æå–æ•°å€¼ã€‚
    
    Args:
        pdf_content (bytes): PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
        pdf_url (str): PDFæ–‡ä»¶çš„URLï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    Returns:
        list: åŒ…å«æå–æ•°æ®çš„å­—å…¸åˆ—è¡¨
    """
    found_items = []
    parent_categories = ["å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"]
    
    def extract_number_from_text(text):
        """ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°å­—"""
        if not text:
            return "ç©ºå€¼", False, False
        
        cleaned_text = text.strip().replace(' ', '')
        
        # æ•°å­—åŒ¹é…æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œæ›´ç²¾ç¡®çš„åœ¨å‰ï¼‰
        number_patterns = [
            r'((?:\d{1,3},)*\d{1,3}\.\d{2})',  # æ ‡å‡†æ ¼å¼ï¼š1,234.56
            r'((?:\d{1,3},)*\d{1,3}\.\d+)',    # å¸¦å°æ•°ç‚¹çš„æ ¼å¼ï¼š1,234.5 æˆ– 1,234.567
            r'((?:\d{1,3},)+\d+)',             # å¸¦åƒåˆ†ä½çš„æ•´æ•°ï¼š1,234,567
            r'((?:\d{1,3},)*\d+)',              # æ•´æ•°æ ¼å¼ï¼š1,234
            r'(\d+\.\d{2})',                    # ç®€å•å°æ•°ï¼š123.45
            r'(\d+\.\d+)',                      # å¸¦å°æ•°ç‚¹çš„æ•°å­—ï¼š123.5
            r'(\d+)',                           # çº¯æ•°å­—ï¼š123ï¼ˆä»»ä½•ä½æ•°ï¼‰
        ]
        
        for pattern in number_patterns:
            match = re.search(pattern, cleaned_text)
            if match:
                value_str = match.group(1)
                try:
                    numeric_value = float(value_str.replace(',', ''))
                    is_positive = numeric_value > 0
                    return value_str, True, is_positive
                except:
                    return value_str, True, True
        
        return "ç©ºå€¼", False, False
    
    try:
        # ä¸´æ—¶æŠ‘åˆ¶pdfplumberçš„è­¦å‘Šå’Œé”™è¯¯è¾“å‡º
        import sys
        from io import StringIO
        
        # æ•è·stderrä»¥æŠ‘åˆ¶pdfplumberçš„è­¦å‘Š
        old_stderr = sys.stderr
        sys.stderr = StringIO()
        
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with pdfplumber.open(BytesIO(pdf_content)) as pdf:
                    for page_num, page in enumerate(pdf.pages, 1):
                        tables = page.extract_tables()
                        if not tables:
                            continue
                        
                        for table in tables:
                            for row_idx, row in enumerate(table):
                                if not row or not row[0]:
                                    continue
                                
                                first_col_text = row[0].replace('\n', '') if row[0] else ''
                                first_col_no_space = first_col_text.replace(' ', '').replace('ã€€', '')
                                
                                # æŸ¥æ‰¾"å…¶ä¸­ï¼šæ•°æ®èµ„æº"ï¼ˆä½¿ç”¨æ­£åˆ™åŒ¹é…ï¼Œå…è®¸å†’å·å˜ä½“å’Œç©ºæ ¼ï¼‰
                                target_pattern = re.compile(r'å…¶ä¸­[ï¼š:ï¼š]\s*æ•°æ®èµ„æº')
                                
                                if target_pattern.search(first_col_text) or target_pattern.search(first_col_no_space):
                                    found_value = "ç©ºå€¼"
                                    has_number = False
                                    
                                    # ä»ç¬¬1åˆ—å¼€å§‹æŸ¥æ‰¾æ•°å€¼ï¼ˆåªåœ¨åŒä¸€è¡ŒæŸ¥æ‰¾ï¼‰
                                    for i in range(1, len(row)):
                                        if row[i]:
                                            value, has_num, is_positive = extract_number_from_text(str(row[i]))
                                            if has_num and value != "ç©ºå€¼" and is_positive:
                                                found_value = value
                                                has_number = True
                                                break
                                    
                                    if has_number and found_value != "ç©ºå€¼":
                                        # å‘ä¸ŠæŸ¥æ‰¾çˆ¶ç±»åˆ«ï¼ˆæ£€æŸ¥ä¸Šé¢çš„è¡Œï¼‰
                                        # å‘ä¸ŠæŸ¥æ‰¾æœ€å¤š2è¡Œ
                                        parent_category = None
                                        for i in range(max(0, row_idx - 2), row_idx):
                                            if i >= 0 and table[i] and table[i][0]:
                                                prev_first_col = str(table[i][0]).replace('\n', '')
                                                for cat in parent_categories:
                                                    if cat in prev_first_col:
                                                        parent_category = cat
                                                        break
                                                if parent_category:
                                                    break
                                        
                                        if parent_category:
                                            found_items.append({
                                                "category": parent_category,
                                                "value": found_value,
                                                "method": "table"  # æ ‡è®°æ¥æº
                                            })
                                            print(f"    âœ… [è¡¨æ ¼] ç¬¬{page_num}é¡µ {parent_category}å…¶ä¸­ï¼šæ•°æ®èµ„æº: {found_value}")
        finally:
            # æ¢å¤stderr
            sys.stderr = old_stderr
    
    except Exception as e:
        print(f"    âš ï¸ è¡¨æ ¼æå–æ–¹æ³•å‡ºé”™: {e}")
        return []
    
    return found_items


def process_pdf_link(row_data, session, headers, folder_path, download_pdf=True):
    """
    å¤„ç†å•ä¸ªPDFé“¾æ¥ï¼Œä¸‹è½½å¹¶è§£ææ•°æ®
    
    Args:
        row_data (dict): CSVè¡Œæ•°æ®ï¼ŒåŒ…å«PDFé“¾æ¥ç­‰ä¿¡æ¯
        session (requests.Session): è¯·æ±‚ä¼šè¯
        headers (dict): è¯·æ±‚å¤´
        folder_path (str): ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„
        download_pdf (bool): æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°
    
    Returns:
        list: è§£æç»“æœåˆ—è¡¨
    """
    pdf_url = row_data.get('PDFé“¾æ¥', '')
    if not pdf_url:
        print(f"  âŒ è·³è¿‡ï¼šæ— PDFé“¾æ¥")
        return []
    
    sec_code = row_data.get('è‚¡ç¥¨ä»£ç ', row_data.get('è¯åˆ¸ä»£ç ', 'æœªçŸ¥ä»£ç '))
    sec_name = row_data.get('å…¬å¸åç§°', 'æœªçŸ¥å…¬å¸')
    report_title = row_data.get('è´¢æŠ¥åç§°', 'æœªçŸ¥æŠ¥å‘Š')
    report_date = row_data.get('æŠ¥å‘Šæ—¥æœŸ', 'æœªçŸ¥æ—¥æœŸ')
    
    # æ¸…ç†å¹¶æ„é€ æ–‡ä»¶å
    report_name_base = f"{sec_name}ï¼š{report_title}_[{report_date}]"
    file_name = re.sub(r'[\\/:*?"<>|]', '_', report_name_base) + ".pdf"
    file_path = os.path.join(folder_path, file_name)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆä»…åœ¨ä¸‹è½½PDFæ¨¡å¼ä¸‹æ£€æŸ¥ï¼‰
    if download_pdf and os.path.exists(file_path):
        print(f"  ğŸ“„ æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¯»å–: {file_name}")
        try:
            with open(file_path, 'rb') as f:
                pdf_content = f.read()
        except Exception as e:
            print(f"  âŒ è¯»å–å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥: {e}")
            return []
    else:
        try:
            print(f"  ğŸ“¥ æ­£åœ¨ä¸‹è½½: {sec_name} - {report_title}")
            response = session.get(pdf_url, headers=headers, timeout=(15, 45))
            response.raise_for_status()
            
            # éªŒè¯æ˜¯å¦ä¸ºPDF
            if 'application/pdf' not in response.headers.get('Content-Type', ''):
                print(f"  âš ï¸ è­¦å‘Š: {file_name} ä¸æ˜¯PDFæ–‡ä»¶ã€‚")
                return []

            pdf_content = response.content

            # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šæ˜¯å¦ä¿å­˜PDFæ–‡ä»¶åˆ°æœ¬åœ°
            if download_pdf:
                os.makedirs(folder_path, exist_ok=True)
                with open(file_path, 'wb') as f:
                    f.write(pdf_content)
                print(f"  âœ… PDFå·²ä¿å­˜: {file_name}")
            else:
                print(f"  ğŸ“Š ä»…è§£ææ•°æ®ï¼Œæœªä¿å­˜PDF: {file_name}")
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
            return []

    # åœ¨å†…å­˜ä¸­è§£æPDFå†…å®¹ - ä½¿ç”¨ä¸¤ç§æ–¹æ³•
    print(f"  ğŸ” ä½¿ç”¨è¡¨æ ¼æå–æ–¹æ³•...")
    extracted_data_table = extract_data_by_table(pdf_content, pdf_url)
    
    print(f"  ğŸ” ä½¿ç”¨æ–‡æœ¬æå–æ–¹æ³•...")
    extracted_data_text, has_data_resource_keyword = extract_data_by_text(pdf_content, pdf_url)
    
    # åˆå¹¶ä¸¤ç§æ–¹æ³•çš„ç»“æœï¼ˆä¸å»é‡ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®ï¼‰
    all_extracted_data = extracted_data_table + extracted_data_text
    print(f"  ğŸ“Š è¡¨æ ¼æ–¹æ³•æ‰¾åˆ°: {len(extracted_data_table)} æ¡ï¼Œæ–‡æœ¬æ–¹æ³•æ‰¾åˆ°: {len(extracted_data_text)} æ¡ï¼Œæ€»è®¡: {len(all_extracted_data)} æ¡")
    
    # å¦‚æœæ–‡æœ¬æ–¹æ³•æ²¡æœ‰æ£€æµ‹åˆ°"æ•°æ®èµ„æº"ï¼Œå†æ£€æŸ¥è¡¨æ ¼æ–¹æ³•æå–çš„æ•°æ®
    if not has_data_resource_keyword:
        # æ£€æŸ¥å·²æå–çš„æ•°æ®ä¸­æ˜¯å¦æœ‰åŒ…å«"æ•°æ®èµ„æº"çš„ï¼ˆæ¯”å¦‚è¡¨æ ¼æ–¹æ³•æå–åˆ°çš„ï¼‰
        if all_extracted_data:
            has_data_resource_keyword = True
    
    # å°†æŠ¥å‘Šè‡ªèº«ä¿¡æ¯æ·»åŠ åˆ°æå–ç»“æœä¸­
    results_for_excel = []
    if all_extracted_data:
        for item in all_extracted_data:
            results_for_excel.append({
                "è¯åˆ¸ä»£ç ": sec_code,
                "å…¬å¸åç§°": sec_name,
                "æŠ¥å‘Šåç§°": report_title,
                "æŠ¥å‘Šæ—¥æœŸ": report_date,
                "é¡¹ç›®åç§°": item['category'],
                "é‡‘é¢": item['value'],
                "PDFé“¾æ¥": pdf_url,
                "_has_data_resource": 1 if has_data_resource_keyword else 0  # ä¸´æ—¶å­—æ®µï¼Œç”¨äºåç»­åˆ¤æ–­
            })
    else:
        # å³ä½¿æ²¡æ‰¾åˆ°æ•°æ®ï¼Œä¹Ÿè®°å½•ä¸‰æ¡ï¼ˆå¯¹åº”ä¸‰ä¸ªé¡¹ç›®ï¼‰ï¼Œæ–¹ä¾¿è¿½æº¯ï¼Œé‡‘é¢è®¾ä¸º0
        for category in ["å­˜è´§", "æ— å½¢èµ„äº§", "å¼€å‘æ”¯å‡º"]:
            results_for_excel.append({
                "è¯åˆ¸ä»£ç ": sec_code,
                "å…¬å¸åç§°": sec_name,
                "æŠ¥å‘Šåç§°": report_title,
                "æŠ¥å‘Šæ—¥æœŸ": report_date,
                "é¡¹ç›®åç§°": category,
                "é‡‘é¢": "0",
                "PDFé“¾æ¥": pdf_url,
                "_has_data_resource": 1 if has_data_resource_keyword else 0  # ä¸´æ—¶å­—æ®µï¼Œç”¨äºåç»­åˆ¤æ–­
            })
            
    return results_for_excel


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="æŠ¥å‘Šä¿¡æ¯æ”¶é›†å™¨ - ä»CSVæ–‡ä»¶è¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®èµ„æºä¿¡æ¯"
    )
    parser.add_argument(
        "--csv-file",
        type=str,
        default=None,
        help="æŒ‡å®šCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„listed_companies_*.csvæ–‡ä»¶"
    )
    parser.add_argument(
        "--no-download",
        action="store_true",
        help="ä¸ä¸‹è½½PDFæ–‡ä»¶ï¼Œä»…è§£ææ•°æ®ç”ŸæˆExcelï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šæ­¤å‚æ•°ï¼Œç¨‹åºä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½"
    )
    parser.add_argument(
        "--download-pdf",
        action="store_true",
        help="ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå®Œæ•´æ¨¡å¼ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šæ­¤å‚æ•°ï¼Œç¨‹åºä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½"
    )
    return parser.parse_args()


def find_csv_file(csv_file_path=None):
    """
    æŸ¥æ‰¾ç¬¦åˆå‘½åæ¨¡å¼çš„CSVæ–‡ä»¶å¹¶è§£ææ–‡ä»¶åä¿¡æ¯
    
    Args:
        csv_file_path (str, optional): æŒ‡å®šçš„CSVæ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæä¾›ï¼Œç›´æ¥ä½¿ç”¨è¯¥æ–‡ä»¶ï¼›å¦åˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
    
    Returns:
        tuple: (CSVæ–‡ä»¶è·¯å¾„, è§£æä¿¡æ¯å­—å…¸) æˆ– (None, None)
    """
    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
    if csv_file_path:
        if not os.path.exists(csv_file_path):
            print(f"âŒ æŒ‡å®šçš„CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
            return None, None
        print(f"ğŸ“„ ä½¿ç”¨æŒ‡å®šçš„CSVæ–‡ä»¶: {csv_file_path}")
        latest_file = csv_file_path
    else:
        # æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„CSVæ–‡ä»¶
        pattern = "listed_companies_*_*.csv"
        csv_files = glob.glob(pattern)
        
        if not csv_files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆå‘½åæ¨¡å¼çš„CSVæ–‡ä»¶ï¼ˆlisted_companies_*_*.csvï¼‰")
            return None, None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°çš„ï¼ˆæ›´å‡†ç¡®ï¼‰
        def extract_timestamp_from_filename(filename):
            """ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³ç”¨äºæ’åº"""
            try:
                name_without_ext = os.path.basename(filename).replace('.csv', '')
                parts = name_without_ext.split('_')
                # æ–‡ä»¶åæ ¼å¼ï¼šlisted_companies_{start_date}_{end_date}_{report_type}_{timestamp}
                # timestamp æ ¼å¼é€šå¸¸æ˜¯ YYYYMMDD_HHMMSS
                if len(parts) >= 6:
                    timestamp_str = '_'.join(parts[5:])  # è·å–æ—¶é—´æˆ³éƒ¨åˆ†
                    # å°è¯•è§£ææ—¶é—´æˆ³
                    if '_' in timestamp_str:
                        date_part, time_part = timestamp_str.split('_', 1)
                        # è½¬æ¢ä¸ºå¯æ¯”è¾ƒçš„æ ¼å¼
                        return f"{date_part}_{time_part}"
                # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºå¤‡é€‰
                return str(os.path.getmtime(filename))
            except:
                return str(os.path.getmtime(filename))
        
        csv_files.sort(key=extract_timestamp_from_filename, reverse=True)
        latest_file = csv_files[0]
        
        print(f"ğŸ“„ æ‰¾åˆ°CSVæ–‡ä»¶: {latest_file}")
        if len(csv_files) > 1:
            print(f"   æç¤º: æ‰¾åˆ° {len(csv_files)} ä¸ªåŒ¹é…æ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°çš„ï¼ˆæŒ‰æ–‡ä»¶åæ—¶é—´æˆ³ï¼‰: {latest_file}")
            print(f"   å…¶ä»–æ–‡ä»¶: {', '.join(csv_files[1:3])}..." if len(csv_files) > 3 else f"   å…¶ä»–æ–‡ä»¶: {', '.join(csv_files[1:])}")
    
    # è§£ææ–‡ä»¶åï¼šlisted_companies_{start_date}_{end_date}_{report_type}_{timestamp}.csv
    file_name = os.path.basename(latest_file)
    # å»æ‰æ‰©å±•å
    name_without_ext = file_name.replace('.csv', '')
    # åˆ†å‰²æ–‡ä»¶å
    parts = name_without_ext.split('_')
    
    if len(parts) >= 5:
        # listed_companies_{start_date}_{end_date}_{report_type}_{timestamp}
        start_date_str = parts[2]  # ä¾‹å¦‚: 20250801
        end_date_str = parts[3]    # ä¾‹å¦‚: 20250831
        report_type = parts[4]     # ä¾‹å¦‚: bndbg
        # timestamp å¯èƒ½åŒ…å«ä¸‹åˆ’çº¿ï¼Œæ‰€ä»¥å–å‰©ä½™éƒ¨åˆ†
        timestamp = '_'.join(parts[5:]) if len(parts) > 5 else ''
        
        file_info = {
            'start_date_str': start_date_str,
            'end_date_str': end_date_str,
            'report_type': report_type,
            'original_timestamp': timestamp
        }
        
        print(f"ğŸ“‹ è§£ææ–‡ä»¶åä¿¡æ¯:")
        print(f"   å¼€å§‹æ—¥æœŸ: {start_date_str}")
        print(f"   ç»“æŸæ—¥æœŸ: {end_date_str}")
        print(f"   æŠ¥å‘Šç±»å‹: {report_type}")
        
        return latest_file, file_info
    else:
        print(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶åæ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å‘½å")
        return latest_file, None


def pivot_to_wide_format(df_long):
    """
    å°†é•¿æ ¼å¼æ•°æ®è½¬æ¢ä¸ºå®½æ ¼å¼ï¼Œå¹¶æ·»åŠ "æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"åˆ—
    å»é‡é€»è¾‘ï¼š
    1. åŒä¸€ä¸ªè¯åˆ¸ä»£ç ä¸‹çš„å­˜è´§/æ— å½¢èµ„äº§/å¼€å‘æ”¯å‡ºï¼Œä¼˜å…ˆå–å¤§äº0çš„å€¼
    2. å¦‚æœæœ‰å¤šä¸ªå€¼ä¸”ç›¸ç­‰ï¼Œå–ç¬¬ä¸€ä¸ª
    3. å¦‚æœæœ‰å¤šä¸ªå€¼ä¸”ä¸ç›¸ç­‰ï¼Œå–ç¬¬ä¸€ä¸ªå¤§äº0çš„ï¼ˆå¦‚æœéƒ½æ²¡æœ‰å¤§äº0çš„ï¼Œå–ç¬¬ä¸€ä¸ªï¼‰
    
    Args:
        df_long (pd.DataFrame): é•¿æ ¼å¼æ•°æ®
    
    Returns:
        pd.DataFrame: å®½æ ¼å¼æ•°æ®
    """
    print("\næ­£åœ¨è¿›è¡Œæ•°æ®é€è§†æ“ä½œ...")
    
    # å»é‡é€»è¾‘ï¼šæŒ‰è¯åˆ¸ä»£ç å’Œé¡¹ç›®åç§°åˆ†ç»„ï¼Œä¼˜å…ˆé€‰æ‹©å¤§äº0çš„å€¼
    print("æ­£åœ¨æŒ‰è§„åˆ™å»é‡...")
    
    def get_numeric_value(val):
        """å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ï¼Œç”¨äºæ¯”è¾ƒ"""
        if pd.isna(val):
            return 0
        try:
            val_str = str(val).strip().replace(',', '').replace(' ', '')
            if val_str in ['N/A', 'ç©ºå€¼', '-', 'nan', 'None', '0', '']:
                return 0
            return float(val_str)
        except:
            return 0
    
    # æŒ‰è¯åˆ¸ä»£ç ã€å…¬å¸åç§°ã€æŠ¥å‘Šåç§°ã€æŠ¥å‘Šæ—¥æœŸã€PDFé“¾æ¥ã€é¡¹ç›®åç§°åˆ†ç»„
    deduplicated_rows = []
    
    grouped = df_long.groupby(['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ', 'PDFé“¾æ¥', 'é¡¹ç›®åç§°'])
    
    for (sec_code, company, report, date, pdf_link, category), group in grouped:
        if len(group) == 1:
            # åªæœ‰ä¸€æ¡è®°å½•ï¼Œç›´æ¥æ·»åŠ 
            deduplicated_rows.append(group.iloc[0].to_dict())
        else:
            # å¤šæ¡è®°å½•ï¼ŒæŒ‰è§„åˆ™é€‰æ‹©
            group = group.copy()
            group['_numeric_value'] = group['é‡‘é¢'].apply(get_numeric_value)
            
            # ä¼˜å…ˆé€‰æ‹©å¤§äº0çš„å€¼
            positive_rows = group[group['_numeric_value'] > 0]
            
            if len(positive_rows) > 0:
                # å¦‚æœæœ‰å¤§äº0çš„å€¼ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                selected_row = positive_rows.iloc[0]
            else:
                # å¦‚æœæ²¡æœ‰å¤§äº0çš„å€¼ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                selected_row = group.iloc[0]
            
            deduplicated_rows.append(selected_row.drop('_numeric_value').to_dict())
    
    df_long_dedup = pd.DataFrame(deduplicated_rows)
    print(f"å»é‡å‰: {len(df_long)} è¡Œï¼Œå»é‡å: {len(df_long_dedup)} è¡Œ")
    
    # åˆ›å»ºé‡‘é¢é€è§†è¡¨
    df_pivot = df_long_dedup.pivot_table(
        index=['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ', 'PDFé“¾æ¥'], 
        columns='é¡¹ç›®åç§°',                           
        values='é‡‘é¢',                                
        aggfunc='first'                               
    ).reset_index()
    
    print("æ•°æ®é€è§†å®Œæˆï¼")
    
    # åˆ›å»º"æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"åˆ—
    # æ–°é€»è¾‘ï¼šåªè¦PDFä¸­æœ‰"æ•°æ®èµ„æº"è¿™ä¸ªè¯ï¼Œå°±è®¾ä¸º1
    # ä»åŸå§‹é•¿æ ¼å¼æ•°æ®ä¸­è·å–æ¯ä¸ªPDFçš„æ ‡è®°
    has_data_col = []
    item_cols = ['å­˜è´§', 'æ— å½¢èµ„äº§', 'å¼€å‘æ”¯å‡º']  # å®šä¹‰é¡¹ç›®åˆ—ï¼Œç”¨äºåç»­æ•°æ®æ¸…ç†
    
    # ä¸ºæ¯ä¸ªPDFé“¾æ¥åˆ›å»ºä¸€ä¸ªæ ‡è®°å­—å…¸
    pdf_has_data_resource = {}
    if '_has_data_resource' in df_long_dedup.columns:
        for pdf_link in df_long_dedup['PDFé“¾æ¥'].unique():
            pdf_rows = df_long_dedup[df_long_dedup['PDFé“¾æ¥'] == pdf_link]
            if len(pdf_rows) > 0:
                # å–ç¬¬ä¸€æ¡è®°å½•çš„æ ‡è®°ï¼ˆæ‰€æœ‰è®°å½•çš„æ ‡è®°åº”è¯¥ç›¸åŒï¼‰
                pdf_has_data_resource[pdf_link] = int(pdf_rows.iloc[0]['_has_data_resource'])
            else:
                pdf_has_data_resource[pdf_link] = 0
    else:
        # å¦‚æœä¸´æ—¶å­—æ®µä¸å­˜åœ¨ï¼Œé»˜è®¤éƒ½æ˜¯0ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
        for pdf_link in df_pivot['PDFé“¾æ¥'].unique():
            pdf_has_data_resource[pdf_link] = 0
    
    # æ ¹æ®PDFé“¾æ¥è®¾ç½®"æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§"
    for idx, row in df_pivot.iterrows():
        pdf_link = row['PDFé“¾æ¥']
        has_data = pdf_has_data_resource.get(pdf_link, 0)
        has_data_col.append(has_data)
    
    df_pivot['æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§'] = has_data_col
    
    # å°†æ‰€æœ‰ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0
    print("æ­£åœ¨æ¸…ç†æ•°æ®ï¼šå°†ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0...")
    for col in item_cols:
        if col in df_pivot.columns:
            df_pivot[col] = df_pivot[col].replace(['N/A', 'ç©ºå€¼', '-', 'nan', 'None', ''], '0')
            df_pivot[col] = df_pivot[col].fillna('0')
    
    # è°ƒæ•´åˆ—é¡ºåºï¼šåŸºæœ¬ä¿¡æ¯ -> é‡‘é¢åˆ— -> æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§ -> PDFé“¾æ¥
    base_cols = ['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ']
    amount_cols = [col for col in item_cols if col in df_pivot.columns]
    other_cols = ['æ˜¯å¦åŒ…å«æ•°æ®èµ„äº§', 'PDFé“¾æ¥']
    
    final_columns = base_cols + amount_cols + other_cols
    # åªä¿ç•™å­˜åœ¨çš„åˆ—
    final_columns = [col for col in final_columns if col in df_pivot.columns]
    
    df_final = df_pivot[final_columns]
    
    return df_final


def main():
    """
    ä¸»å‡½æ•° - ä»CSVè¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šæ˜¯å¦ä¸‹è½½PDF
    if args.no_download:
        download_pdf = False
        print("\nâœ… å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ï¼šä»…ç”ŸæˆExcelæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼Œä¸ä¸‹è½½PDFï¼‰")
    elif args.download_pdf:
        download_pdf = True
        print("\nâœ… å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ï¼šä¸‹è½½PDFå¹¶ç”ŸæˆExcelï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
    else:
        # è¯¢é—®æ˜¯å¦ä¸‹è½½PDF
        print("\n" + "="*60)
        print("æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°ï¼Ÿ")
        print("y - ä¸‹è½½PDFå¹¶ç”ŸæˆExcelï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
        print("n - ä»…ç”ŸæˆExcelæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼Œä¸ä¸‹è½½PDFï¼‰")
        print("="*60)
        
        while True:
            choice = input("è¯·è¾“å…¥é€‰æ‹© (y/n): ").strip().lower()
            if choice == 'y':
                download_pdf = True
                print("âœ… å·²é€‰æ‹©ï¼šä¸‹è½½PDFå¹¶ç”ŸæˆExcelï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
                break
            elif choice == 'n':
                download_pdf = False
                print("âœ… å·²é€‰æ‹©ï¼šä»…ç”ŸæˆExcelæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ y æˆ– n")
    
    print(f"\nğŸ“ PDFä¸‹è½½æ¨¡å¼: {'å¼€å¯' if download_pdf else 'å…³é—­'}")
    if not download_pdf:
        print("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šä»…è§£æPDFå†…å®¹ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°")
    else:
        print("ğŸ’¾ å®Œæ•´æ¨¡å¼ï¼šä¸‹è½½å¹¶ä¿å­˜PDFæ–‡ä»¶åˆ°æœ¬åœ°")
    
    # æŸ¥æ‰¾CSVæ–‡ä»¶
    csv_file, file_info = find_csv_file(args.csv_file)
    if not csv_file:
        return
    
    # è¯»å–CSVæ–‡ä»¶
    try:
        print(f"\nğŸ“– æ­£åœ¨è¯»å–CSVæ–‡ä»¶: {csv_file}")
        df_csv = pd.read_csv(csv_file, dtype=str)
        print(f"âœ… æˆåŠŸè¯»å– {len(df_csv)} æ¡è®°å½•")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['PDFé“¾æ¥']
        missing_cols = [col for col in required_cols if col not in df_csv.columns]
        if missing_cols:
            print(f"âŒ CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
            return
        
        # æ˜¾ç¤ºåˆ—å
        print(f"ğŸ“‹ CSVæ–‡ä»¶åŒ…å«çš„åˆ—: {', '.join(df_csv.columns.tolist())}")
        
    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # åˆå§‹åŒ–
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Accept": "application/pdf, application/json, text/plain, */*",
        "Referer": "http://www.cninfo.com.cn/new/commonUrl?url=disclosure/list/notice",
    }
    
    folder_path = os.path.join(os.getcwd(), "FinancialReports_Collection")
    all_results_for_excel = []
    start_time = time.time()
    
    print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(df_csv)} ä¸ªPDFé“¾æ¥...")
    print("="*60)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_row = {
            executor.submit(process_pdf_link, row.to_dict(), session, headers, folder_path, download_pdf): idx 
            for idx, row in df_csv.iterrows()
        }
        
        completed = 0
        for future in as_completed(future_to_row):
            completed += 1
            try:
                extracted_data = future.result()
                if extracted_data:
                    all_results_for_excel.extend(extracted_data)
                print(f"ğŸ“Š è¿›åº¦: {completed}/{len(df_csv)} ({completed/len(df_csv)*100:.1f}%)")
            except Exception as exc:
                print(f'âŒ ä¸€ä¸ªä»»åŠ¡åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {exc}')
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ¯ å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"  ğŸ“Š æ€»è®°å½•æ•°: {len(df_csv)}")
    # ç»Ÿè®¡æˆåŠŸæå–çš„æ•°æ®ï¼ˆé‡‘é¢å¤§äº0çš„è®°å½•æ•°ï¼‰
    success_count = 0
    for r in all_results_for_excel:
        amount = r.get('é‡‘é¢', '0')
        if amount and str(amount) not in ['0', 'N/A', 'ç©ºå€¼', '-', 'nan', 'None', '']:
            try:
                if float(str(amount).replace(',', '')) > 0:
                    success_count += 1
            except:
                pass
    print(f"  âœ… æˆåŠŸæå–æ•°æ®ï¼ˆé‡‘é¢>0ï¼‰: {success_count}")
    
    # ç”Ÿæˆæœ€ç»ˆçš„ExcelæŠ¥å‘Š
    print("\n===== æ­£åœ¨ç”ŸæˆExcelæŠ¥å‘Š... =====")
    if all_results_for_excel:
        # ç”Ÿæˆé•¿æ ¼å¼æŠ¥å‘Š
        df_long = pd.DataFrame(all_results_for_excel)
        # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—ï¼ˆä¸å»é‡ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®ï¼‰
        # æ³¨æ„ï¼šå¿…é¡»ä¿ç•™_has_data_resourceå­—æ®µï¼Œä¾›pivot_to_wide_formatä½¿ç”¨
        required_cols = ['è¯åˆ¸ä»£ç ', 'å…¬å¸åç§°', 'æŠ¥å‘Šåç§°', 'æŠ¥å‘Šæ—¥æœŸ', 'é¡¹ç›®åç§°', 'é‡‘é¢', 'PDFé“¾æ¥']
        if '_has_data_resource' in df_long.columns:
            required_cols.append('_has_data_resource')
        available_cols = [col for col in required_cols if col in df_long.columns]
        df_long = df_long[available_cols]
        
        # å°†æ‰€æœ‰ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0ï¼ˆé•¿æ ¼å¼ä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œä¸å»é‡ï¼‰
        print("æ­£åœ¨æ¸…ç†æ•°æ®ï¼šå°†ç©ºå€¼ã€N/Aç­‰æ›¿æ¢ä¸º0...")
        df_long['é‡‘é¢'] = df_long['é‡‘é¢'].replace(['N/A', 'ç©ºå€¼', '-', 'nan', 'None', ''], '0')
        df_long['é‡‘é¢'] = df_long['é‡‘é¢'].fillna('0')
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºtimestamp
        output_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if file_info:
            # ä»CSVæ–‡ä»¶åä¸­æå–çš„ä¿¡æ¯
            start_date_str = file_info['start_date_str']
            end_date_str = file_info['end_date_str']
            report_type = file_info['report_type']
            
            # æ„å»ºæ–‡ä»¶åï¼šlong_output_{start_date}_{end_date}_{report_type}_{timestamp}.xlsx
            long_output_filename = f'long_output_{start_date_str}_{end_date_str}_{report_type}_{output_timestamp}.xlsx'
            wide_output_filename = f'wide_output_{start_date_str}_{end_date_str}_{report_type}_{output_timestamp}.xlsx'
        else:
            # å¦‚æœæ— æ³•è§£ææ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤å‘½å
            long_output_filename = f'long_output_{output_timestamp}.xlsx'
            wide_output_filename = f'wide_output_{output_timestamp}.xlsx'
        
        # ç”Ÿæˆé•¿æ ¼å¼Excel
        if os.path.exists(long_output_filename):
            os.remove(long_output_filename)
        df_long.to_excel(long_output_filename, index=False)
        print(f"ğŸ‰ é•¿æ ¼å¼æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜ä¸º ./{long_output_filename}")
        
        # ç”Ÿæˆå®½æ ¼å¼æŠ¥å‘Š
        print("\næ­£åœ¨ç”Ÿæˆå®½æ ¼å¼æŠ¥å‘Š...")
        df_wide = pivot_to_wide_format(df_long)
        
        if os.path.exists(wide_output_filename):
            os.remove(wide_output_filename)
        
        df_wide.to_excel(wide_output_filename, index=False, freeze_panes=(1, 0))
        print(f"ğŸ‰ å®½æ ¼å¼æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜ä¸º ./{wide_output_filename}")
        
        print("\nğŸ“Š å®½æ ¼å¼æŠ¥å‘Šé¢„è§ˆ:")
        print(df_wide.head(10))
    else:
        print("âŒ æœªæå–åˆ°ä»»ä½•æ•°æ®ï¼Œä¸ç”ŸæˆExcelæ–‡ä»¶ã€‚")
        
    end_time = time.time()
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {(end_time - start_time):.2f} ç§’")
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    print("=" * 60)
    print("æŠ¥å‘Šä¿¡æ¯æ”¶é›†å™¨ - æ•°æ®èµ„æºæå–å·¥å…·")
    print("=" * 60)
    print("åŠŸèƒ½ï¼šä»CSVæ–‡ä»¶è¯»å–PDFé“¾æ¥å¹¶æå–æ•°æ®èµ„æºä¿¡æ¯")
    print("è¾“å‡ºï¼šç”Ÿæˆé•¿æ ¼å¼å’Œå®½æ ¼å¼çš„ExcelæŠ¥å‘Š")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•ï¼š")
    print("  python report_info_collection.py                              # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„CSVæ–‡ä»¶ï¼Œä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½PDF")
    print("  python report_info_collection.py --csv-file file.csv          # æŒ‡å®šCSVæ–‡ä»¶ï¼Œä¼šè¯¢é—®æ˜¯å¦ä¸‹è½½PDF")
    print("  python report_info_collection.py --no-download                 # ä¸ä¸‹è½½PDFï¼Œä»…è§£ææ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
    print("  python report_info_collection.py --download-pdf                # ä¸‹è½½PDFåˆ°æœ¬åœ°ï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")
    print("  python report_info_collection.py --csv-file file.csv --no-download  # æŒ‡å®šCSVæ–‡ä»¶ä¸”ä¸ä¸‹è½½PDF")
    print("=" * 60)
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nç¨‹åºæ‰§è¡Œå®Œæ¯•")

